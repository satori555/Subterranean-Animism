## 优化方法

- ### 梯度下降法：


没啥好说的
$$
x_{n+1}=x_n-\eta\nabla f(x)
$$
其中$\eta$为学习率。

#### 基于梯度下降法的变式：

SGD：随机梯度下降，随机选一条或多条数据（mini-batch）进行梯度下降。

Momentum：为了减少震荡，每次更新的时候加上上一次更新的向量（乘一个0到1的系数），使得两次更新方向相同时加速，方向相反时减速。

AdaGrad：动态调整学习率，让全局学习率除以历史梯度的平方和再开根号，学习率逐步下降。

RMSProp：AdaGrad的改进版，梯度平方累加时加一个调整系数。

Adam：Momentum和RMSProp的结合。

- ### 牛顿法：


牛顿法可用于求解方程$g(x)=0$。

选取$x_0$作为初始值，过点$(x_0,g(x_0))$作切线$L$：
$$
L:y=g(x_0)+g'(x_0)(x-x_0)
$$
找到$L$与x轴的交点$x_1$：
$$
x_1=x_0-\frac{g(x_0)}{g'(x_0)}
$$
重复以上步骤，直到结果收敛。递推公式为：
$$
x_{n+1}=x_n-\frac{g(x_n)}{g'(x_n)}
$$
推广：求解无约束最优化问题：$\min\limits_x f(x)$

二阶泰勒展开：
$$
f(x)=f(x_k)+f'(x)(x-x_k)+\frac 12f''(x_k)(x-x_k)^2
$$
极小值满足$f'(x)=0$，即
$$
f'(x_k)+f''(x_k)(x-x_k)=0
$$

$$
x_{k+1}=x_k-\frac{f'(x_k)}{f''(x_k)} \equiv x_k-\frac{g_k}{h_k}
$$

推广到$\vec{x}$是多维向量的情况：
$$
\vec{g}=\left[\frac{\part f}{\part x_i}\right]
$$
$H$是Hessian矩阵：
$$
H=\left[\frac{\part^2f}{\part x_i\part x_j}\right]
$$
迭代公式变为：
$$
\vec x_{k+1} = \vec x_k - H_k^{-1}\vec g_k
$$

#### 牛顿法改进：

阻尼牛顿法：优化步长。

拟牛顿法：为了避免对Hessian矩阵求逆，尝试用一个不含二阶导数的矩阵$U$来代替$H^{-1}$，具体构造方法有DFP法，BFGS法，L-BFGS法等。



