# 逻辑回归（对数几率回归）

### 基础

考虑二分类任务，把线性回归模型的预测值z转换为0或1，例如单位阶跃函数，若z>0判为正例，若z<0判为反例。由于阶跃函数不连续，可以采用对数几率函数(logistic function)，也就是sigmoid函数：
$$
\begin{align}
y &= \frac1{1+e^{-z}} \\
  &= \frac1{1+e^{-w^Tx+b}}
\end{align}
$$
（注意x和w是向量）
$$
\ln\frac y{1-y} = w^Tx+b
$$
将y视为x为正例的可能性$p(y=1|x)$，则1-y就是反例的可能性$p(y=0|x)$，那么
$$
\ln\frac{p(y=1|x)}{p(y=0|x)}=w^Tx+b
$$
定义对数几率(log odds，或logit)：
$$
\text{logit}(y)=\log\frac y{1-y}
$$
正例和反例的概率为：
$$
p(y=1|x)\equiv \pi(x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}} \\
p(y=0|x)\equiv 1-\pi(x)=\frac1{1+e^{w^Tx+b}}
$$


给定数据集$\{\vec{x}_i,y_i\}$，使用极大似然法来估计参数。似然函数为：
$$
\prod_{i=1}^N[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}
$$
对数似然函数：
$$
\begin{align}
L(w) &=\sum_{i=1}^N[y_i\log\pi(x_i)+(1-y_i)log(1-\pi(x_i))] \\
&=\sum_{i=1}^N\left[ y_i\log\frac{\pi(x_i)}{1-\pi(x_i)}+\log(1-\pi(x_i)) \right] \\
&=\sum_{i=1}^N\left[ y_i(wx_i+b)-\log(1+e^{(wx_i+b)}) \right]
\end{align}
$$
其中$wx_i\equiv\sum_j w_jxji$

对$L(w)$求极大值得到$w$的估计值。



参考：

[1] 逻辑回归(logistics regression)_激进的蜗牛-CSDN博客
https://blog.csdn.net/weixin_39445556/article/details/83930186



### 进阶

为什么要使用sigmoid函数？逻辑回归输出的值是真实的概率吗？

指数族分布：
$$
P(y;\eta)=b(y)\exp(\eta^T T(y)-a(\eta))
$$
设 $y$ 服从伯努利分布
$$
\begin{align}
P(y;\phi)&=\phi^y(1-\phi)^{1-y} \\
&=\exp(y\log\phi+(1-y)\log(1-\phi)) \\
&=\exp\left[ y \left( \log(\frac{\phi}{1-\phi}) \right) + \log(1-\phi) \right]
\end{align}
$$
对比指数族分布的定义，有
$$
\begin{align}
b(y) &= 1 ,\quad T(y) = y \\
a(\eta) &= -\log(1-\phi) = \log(1+e^\eta) \\
\eta &= \log(\frac{\phi}{1-\phi}) => \phi=\frac1{1+e^{-\eta}} \\
\end{align}
$$
根据广义线性模型假设，有
$$
h_\Theta(x)=E(y|x;\theta)=\phi=\frac1{1+e^{-\theta^Tx}}
$$
从而得到我们的预测目标即sigmoid函数，用来作为后验概率 $P(y=1|x)$ 。

如果问题满足假设，$y$ 服从伯努利分布，$\eta$ 与 $x$ 具有线性关系，那么逻辑回归的输出就是真实的概率。



参考：

[1] 机器学习面试之逻辑回归输出的值是真实的概率吗？ - 简书
https://www.jianshu.com/p/a8d6b40da0cf

[2] 解释logistic回归为什么要使用sigmoid函数_qq_19645269的博客-CSDN博客
https://blog.csdn.net/qq_19645269/article/details/79551576

[3] 广义线性模型(Generalized Linear Model)——机器学习_Seth的博客-CSDN博客
https://blog.csdn.net/a493823882/article/details/81477235