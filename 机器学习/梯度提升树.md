# 梯度提升树

提升树是以决策树为基本分类器的提升方法，被认为是统计学习中性能最好的方法之一。

## 从CART到XGBoost

1. CART
2. Boosting
3. Gradient Boosting Decision Tree (GBDT)
4. Extreme Gradient Boosting (XGBoost)

## Boosting

提升树模型可以表示为决策树的加法模型：
$$
f_M(x)=\sum_mT(x;\Theta_m)
$$
其中 $T(x;\Theta_m)$ 表示决策树，简记为 $T_m(x)$，$\Theta_m$ 为决策树的参数，$M$ 为树的个数。

提升树采用前向分步算法。首先确定初始提升树 $f_0(x)=0$ ，一步步优化模型：
$$
f_1(x)=f_0(x)+T_1(x) \\
f_2(x)=f_1(x)+T_2(x) \\
... \\
f_m(x)=f_{m-1}(x)+T_m(x) \\
f_M(x)=\sum_{m=1}^MT_m(x)
$$
每一步我们希望通过使损失函数极小化，确定下一棵决策树的参数 $\Theta_m$：
$$
\hat\Theta_m = \arg\min_{\Theta_m} L(y,f_{m-1}(x)+T_m(x))
$$
如果采用平方误差函数：
$$
L(y,f(x))=(y-f(x))^2
$$
损失函数为：
$$
\begin{align}
L(y,f_{m-1}(x)+T_m(x)) &= [y-f_{m-1}(x)-T_m(x)]^2 \\
&= [r_m-T_m(x)]^2
\end{align}
$$
其中 **残差** 定义为：
$$
r_m=y-f_{m-1}(x)
$$
因此对于平方误差函数，只需要拟合当前模型的残差即可。

## Gradient Boosting

提升树利用加法模型与前向分布算法实现学习优化。但是对于一般损失函数，每一步优化并不那么容易。

类比梯度下降法，如果我们沿着误差函数的负梯度方向走，就能使误差下降。

梯度提升树（GBDT）就是利用损失函数的负梯度在当前模型的值
$$
-\left[\frac{\part L(y,f(x))}{\part f(x)}\right]_{f(x)=f_{m-1}(x)}
$$
作为残差的近似值。

GBDT算法步骤：

训练数据集 $(x_i,y_i) \sim (x,y)$，损失函数 $L(y,f(x))$

+ 初始化 $f_0(x)$ 

+ 对 $m=1,...,M$ ：
  + 计算残差 $r_m$ ，拟合一棵决策树 $T_m(x)$
  + 进行线性搜索确定最优步长 $\eta_m$
  + 更新 $f_m(x)=f_{m-1}(x)+\eta_m T_m(x)$

+ 得到回归树 $f_M(x)=\sum_m\eta_m T_m(x)$ 

## XGBoost

XGBoost与GBDT的损失函数有两处差别，一是使用了二阶导数，二是加入了正则项。

每一步的目标函数为：
$$
O_m=\sum_i[L(y_i,f_{m-1}(x_i)+T_m(x))]+\Omega(T_m)
$$
正则项使得模型避免过拟合
$$
\Omega(T_m)=\gamma T+\frac12\lambda\sum_{j=0}^T w_j^2
$$
其中 $T$ 为叶子数，$w_j$ 是对样本的预测值。

对损失函数使用泰勒展开：
$$
L(y_i,f_{m-1}(x_i)+T_m(x_i)) \\
=L(y_i,f_{m-1}(x_i))+g_iT_m(x_i)+\frac12h_iT_m^2(x_i)+...
$$
保留到二阶，同时舍弃 0 阶项（对求最优解无影响），目标函数为：
$$
\begin{align}
O_m
&= \sum_i[g_iT_m(x_i)+\frac12h_iT_m^2(x_i)]+\gamma T +\frac12\lambda\sum_jw_j^2 \\
&= \sum_i[g_i\sum_jw_j+\frac12h\sum_jw_j^2] +\gamma T +\frac12\lambda\sum_jw_j^2 \\
&= \sum_j[(\sum_{i\in I_j}g_i)w_j + \frac12((\sum_{i\in I_j}h_i)+\lambda)w_j^2]+\gamma T
\end{align}
$$
简化一下式子，令 $G_j=\sum_{i\in I_j}g_i$ ，$H_j=\sum_{i\in I_j}h_i$ 。

对目标函数求导，令其导数为 0 解得：
$$
w_j^*=-\frac{G_j}{H_j+\lambda}
$$
对应的目标函数为：
$$
O=-\frac12\sum_j^T\frac{G_j^2}{H_j+\lambda}+\gamma T
$$

目标函数 $O$ 代表我们指定一个树结构时，在最终目标上最多会减少多少。每次分裂节点时，我们要计算出这个节点分裂出的左子树和右子树的导数的和，和划分前的结构比较，选择变化最大的作为最合适的分割，类似于决策树中的信息增益：
$$
Gain=\frac12\left[
\frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} 
-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}
\right] - \gamma
$$
其中最后一项 $\gamma$ 表示增加叶子节点带来的复杂度代价。



##### 参考：

[1] 李航，统计学习方法

[2] 提升树、GBDT与XGBoost
https://mp.weixin.qq.com/s/ePHvIADfBBhW8hlsmvidrQ

[3] Adaboost, GBDT 与 XGBoost 的区别 - 知乎
https://zhuanlan.zhihu.com/p/42740654