# 决策树

决策树算法：

+ 特征选择
+ 决策树生成
+ 剪枝

划分数据集的大原则是：划分后数据集的纯度变高，或者说不确定度更低。

划分后的纯度为各子集的纯度的加权和。

### 信息增益：ID3算法

信息增益表示的是：得知特征 X 的信息使得 Y 的不确定性减少的程度，也就是互信息。

特征 A 对数据集 D 的信息增益 g(D,A) 定义如下：
$$
g(D,A)=H(D)-H(D|A)
$$
设训练集样本数为 $|D|$，有 $k$ 个类别，每个类别包含的样本数为 $|C_k|$，信息熵为：
$$
H(D)=-\sum_k\frac{|C_k|}{|D|}\log\frac{|C_k|}{|D|}
$$
根据特征 $A$ 可以将数据集 $D$ 划分成 $n$ 个子集 $D_i$ ，条件熵为：
$$
H(D|A)=\sum_i\frac{|D_i|}{|D|}H(D_i)
$$

### 信息增益比：C4.5算法

以信息增益作为特征选择，会偏向于选择取值较多的特征，可以采用信息增益比进行矫正。

特征 A 对训练集 D 的信息增益比定义为：
$$
g_R(D|A)=\frac{g(D,A)}{H_A(D)}
$$
其中
$$
H_A(D)=-\sum_i\frac{|D_i|}{|D|}\log\frac{|D_i|}{|D|}
$$

### 基尼指数：CART算法

CART全称为分类与回归树(classification and regression tree)。

CART算法假设决策树是一个二叉树，它通过递归地二分每个特征，将特征空间划分成有限个单元，并在这些单元上确定预测的概率分布。

##### 分类树

对于分类树，采用基尼指数最小化准则：

分类问题中，假设有 K 个类别，样本点属于第 k 类的概率为 $p_k$ ，则概率分布的基尼指数为：
$$
Gini(p)=\sum_k p_k(1-p_k)=1-\sum_kp_k^2
$$
从定义中可以发现，当 $p_1=p_2=...=p_K$ 时，基尼指数最大，这时候分到每个类的概率是一样的，判别性极低。当某些 $p_i$ 较大时，基尼指数会变小，一维整判别性较高，样本中的类别不平衡。

在给定特征 A 的条件下，样本集合 D 的基尼指数为：
$$
Gini(D,A)=p_1 Gini(D_1)+p_2 Gini(D_2)+...
$$
其中 $p_i=\frac{|D_i|}{|D|}$ 。

在CART分割时，我们按照基尼指数最小来确定分割点的位置。

##### 回归树

回归树预测的输出是连续型变量。在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。

数据集：
$$
D=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}
$$
选择最优切分变量 $j$ 与切分点 $s$：遍历变量 $j$，对规定的切分变量 $j$ 扫描切分点 $s$，选择使下式得到最小值时的 $(j,s)$ 对：
$$
\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2}(y_i-c_2)^2\right]
$$
其中 $R_m$ 是被划分的空间，$c_m$ 是空间 $R_m$ 对应的平均输出值。

假设已将输入空间划分为 $M$ 个单元，每个单元 $R_m$ 上有一个固定的输出值 $c_m$，则回归树可以表示为：
$$
f(x)=\sum_m c_m I(x\in R_m)
$$
当输入空间的划分确定时，可以用平方误差 $\sum_{x\in R_m}(y_i-f(x_i))^2$ 表示回归树的预测误差。

##### 参考：

[1] 深入学习决策树算法原理 - 战争热诚 - 博客园
https://www.cnblogs.com/wj-1314/p/9428494.html

[2] 机器学习之分类与回归树(CART) - 知乎
https://zhuanlan.zhihu.com/p/36108972