# 贝叶斯决策论（Bayes Decision Theory）

## 贝叶斯分类器

考虑抛硬币的例子：我们观察到的硬币尺寸为 X（大、中、小），可能的投掷结果为 Y：正(1)、反(0)。

##### 先验概率：

假如我们对硬币一无所知，我们只能预测正反面概率都是 0.5。又或者这是一种特殊的硬币，出现正面的概率 P(1)=0.8，出现反面的概率 P(0)=0.2，这样我们预测更容易出现正面。这里的 P(1) 和 P(0) 叫做 **先验概率** 。我们在观测（硬币尺寸 X）之前就已知的概率分布。

##### 后验概率：

实际上我们可以观测到硬币的一些属性，而非完全一无所知。因此我们尝试回答：当我观察到硬币大小时，它正面（反面）朝上的概率是多少？也就是估计 P(y=1|X)或 P(y=0|X)。

这里 P(y|X) 叫做 **后验概率** ，指的是观测到 X 后我们对 y 的估计。

后验概率引入了额外的观测信息，所以预测的准确度得到了加强。大部分机器学习模型尝试得到的，就是后验概率。

##### 贝叶斯公式：

后验概率无法直接获得，需要借助贝叶斯公式：
$$
P(y|X)=\frac{P(X|y)P(y)}{P(X)}
$$
等号右边，P(y) 为先验概率，P(X) 为观测结果，P(X|y) 叫做 **类条件概率** ，也可以通过观测获得。

有了后验概率 P(y|X)，就可以对样本进行分类。

## 极大似然估计（maximum likelihood estimation）

实际上由于我们只能获得有限的数据，先验概率 P(y) 和类条件概率 P(X|y) 都是未知的，因此我们要先进行估计。

先验概率的估计比较简单，可以依靠经验，或者用训练数据中各个类别的频率估计。

类条件概率的估计非常难，解决的办法是把估计完全未知的概率分布 P(X|y) 转化为估计参数，可以采用极大似然估计方法。

##### 极大似然估计：

先确定模型（概率分布），模型的参数未知。利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。

假设样本都是独立同分布，考虑样本集 $D(x_1,...,x_N)$ ，估计参数向量 $\theta$ 。

似然函数：
$$
l(\theta)=p(D|\theta)=\prod_i p(x_i|\theta)
$$
求出使该组样本的概率最大的参数 $\theta$ 值：
$$
\hat\theta=\arg\max_\theta l(\theta)
$$
为了避免连乘，定义对数似然函数 $\ln l(\theta)$ ：
$$
\hat\theta=\arg\max_\theta\sum_i\ln p(x_i|\theta)
$$
求导，解似然方程。



参考：

[1] 极大似然估计详解，写的太好了！_qq_39355550的博客-CSDN博客
https://blog.csdn.net/qq_39355550/article/details/81809467

## EM算法

要估计的似然函数包含隐变量 $z$ ：$L(\theta)=\ln p(x,z|\theta)$ 。

EM算法的关键是找到 $L(\theta)$ 的一个下界 $Q(\theta;\theta^n)$ ，然后不断极大化这个下界，从而逼近要求解的似然函数。

EM算法的一般步骤：

+ 随机初始化 $\theta^0$ 
+ 迭代以下两步
  + E步：给出当前的参数估计 $\theta^n$ ，计算似然函数的下界 $Q(\theta;\theta^n)$ 
  + M步：重新估计参数，即求 $\theta^{n+1}$，使得 $\theta^{n+1}=\arg\max_\theta Q(\theta;\theta^n)$ 
+ 直到 $Q(\theta;\theta^n)$ 收敛