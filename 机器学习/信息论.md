# 信息论

### 信息熵

假设随机变量 $X$ 服从概率分布 $P(x)$，当 $X=x$ 时的 **信息量** 或 **编码长度** 称为 **自信息** :
$$
I(x)=-log(p(x))
$$
平均编码长度，即 **信息熵**：
$$
H(x)=E[I(x)]=-\sum_xp(x)\log p(x)
$$
信息熵越大，则随机变量的信息越多，信息熵越低，则信息越少。如当 $X$ 只有一个取值时，$p(x)=1$ ，信息熵为 0 ，信息量也为 0。如果概率分布是一个均匀分布，则信息熵最大。

### 联合熵和条件熵

假设两个随机变量 $X$ 和 $Y$，联合概率分布为 $P(x,y)$，则 **联合熵** 为：
$$
H(X,Y)=-\sum_x\sum_yp(x,y)\log p(x,y)
$$
**条件熵** 定义为 $X$ 给定的条件下，$Y$ 的条件概率的熵对 $X$ 的期望：
$$
\begin{align}
H(Y|X) &= -\sum_x p(x)H(Y|X=x) \\
&= -\sum_xp(x)\sum_y p(y|x)\log p(y|x) \\
&= -\sum_x\sum_y p(x,y)\log\frac{p(x,y)}{p(x)} \\
&= H(X,Y)-H(X)
\end{align}
$$

### 互信息

互信息是衡量已知一个变量时，另一个变量的不确定性减少的程度，定义为：
$$
I(X;Y)=\sum_x\sum_yp(x,y)\log\frac{p(x,y)}{p(x)p(y)}
$$
互信息有以下性质：
$$
I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)
$$
可以看出，互信息描述的是当一个随机变量确定时，另一个随机变量的不确定度下降了多少。

### 交叉熵

根据定义，从分布 $p(x)$ 中随机抽取一个事件，传达这条信息所需的最优平均编码长度为信息熵 $H(p)$ 。如果采用另一种分布 $q(x)$ 的最优编码方式进行编码，则平均编码长度为：
$$
H(p,q)=E_p[-\log q(x)]=-\sum_x p(x)\log q(x)
$$
在给定 $p(x)$ 的情况下，如果 $q$ 和 $p$ 越接近，交叉熵就越小。

### KL散度

KL散度也叫 **KL距离** 或 **相对熵** ，用来描述用概率分布 $q$ 来近似真实分布 $p$ 时造成的信息损失量。

KL散度是按照分布 $q$ 的最优编码对分布 $p$ 的信息进行编码，得到的平均编码长度 $H(p,q)$ 和最优编码 $H(p)$ 之间的差异：
$$
\begin{align}
D_{KL}(p||q) &= H(p,q)-H(p) \\
&= \sum_xp(x)\log \frac{p(x)}{q(x)}
\end{align}
$$
KL散度可以衡量两个分布之间的距离。

### JS散度

JS散度是一种对称的衡量两个分布相似度的度量方式：
$$
D_{JS}(p||q)=\frac12D_{KL}(p||m)+\frac12D_{KL}(q||m)
$$
其中 $m=\frac12(p+q)$ 。