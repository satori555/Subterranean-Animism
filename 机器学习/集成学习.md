# 集成学习

## AdaBoost (Adaptive Boosting)

对提升方法来说，有两个问题需要回答：

1. 在每一轮如何改变训练数据的权值或概率分布？

   AdaBoost：提高那些被前一轮分类器错误分类的样本的权值，降低被正确分类的样本的权值。

2. 如何将弱分类器组合成一个强分类器？

   AdaBoost：加权多数表决，即加大分类错误率小的弱分类器的权值，减小分类错误率大的弱分类器的权值。

AdaBoost训练过程：

数据集 $T=(x_i,y_i)$ ，其中 $y_i=\pm1$ ，输出最终分类器 $G(x)$。

+ 初始化训练数据集的权值为均匀分布 $D_1$ 

+ 对 m = 1, 2, ..., M：
  
  + 使用具有权值分布的训练集学习，得到基本分类器 $G_m$ 
  
  + 计算 $G_m(x)$ 在训练数据集上的分类误差率 $e_m$ 
  
  + 计算 $G_m(x)$ 的系数
    $$
    \alpha_m=\frac12\ln\frac{1-e_m}{e_m}
    $$
  
  + 更新训练集的权值分布（$Z_m$ 为归一化因子）
    $$
    D_{m+1}=(w_{m+1,1},w_{m+1,2},...,w_{m+1,N})
    $$
  
    $$
    w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp(-\alpha_my_iG_m(x_i)),\ i=1,2,...,N
    $$
  
+ 组合得到最终分类器

  $$
  G(x)=\text{sign} \left( \sum_{m=1}^M \alpha_m G_m(x) \right)
  $$

由前向分步算法可以推导出 AdaBoost：
$$
f_m(x)=f_{m-1}(x)+\alpha_m G_m(x)
$$
目标函数使用指数损失：
$$
(\alpha_m,G_m(x))
= \arg\min_{\alpha,G} \sum_i \exp[-y_i(f_{m-1}(x)+\alpha G(x_i))]
$$
注意到 $f_{m-1}$ 和 当前 $m$ 轮训练无关：
$$
\begin{align}
\sum_i\exp[y_i\alpha G(x)]
&= \sum_{y_i=G_m(x_i)}e^{-\alpha}+\sum_{y_i\ne G_m(x_i)}e^{\alpha} \\
&= (1-e_m)e^{-\alpha} + e_me^{\alpha}
\end{align}
$$
为了使损失函数最小，对 $\alpha$ 求导并使导数为 0 ：
$$
-(1-e_m)e^{-\alpha} + e_me^{\alpha} = 0
$$
得：
$$
\alpha_m = \frac12\ln\frac{1-e_m}{e_m}
$$
参考：李航：统计学习方法


## 随机森林