# 集成学习

## AdaBoost (Adaptive Boosting)

对提升方法来说，有两个问题需要回答：

1. 在每一轮如何改变训练数据的权值或概率分布？

   AdaBoost：提高那些被前一轮分类器错误分类的样本的权值，降低被正确分类的样本的权值。

2. 如何将弱分类器组合成一个强分类器？

   AdaBoost：加权多数表决，即加大分类错误率小的弱分类器的权值，减小分类错误率大的弱分类器的权值。

AdaBoost训练过程：

数据集 $T=(x_i,y_i)$ ，其中 $y_i=\pm1$ ，输出最终分类器 $G(x)$。

+ 初始化训练数据集的权值为均匀分布 $D_1$ 

+ 对 m = 1, 2, ..., M：
  
  + 使用具有权值分布的训练集学习，得到基本分类器 $G_m$ 
  
  + 计算 $G_m(x)$ 在训练数据集上的分类误差率 $e_m$ 
  
  + 计算 $G_m(x)$ 的系数
    $$
    \alpha_m=\frac12\ln\frac{1-e_m}{e_m}
    $$
  
  + 更新训练集的权值分布（$Z_m$ 为归一化因子）
    $$
    D_{m+1}=(w_{m+1,1},w_{m+1,2},...,w_{m+1,N})
    $$
  
    $$
    w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp(-\alpha_my_iG_m(x_i)),\ i=1,2,...,N
    $$
  
+ 组合得到最终分类器

  $$
  G(x)=\text{sign} \left( \sum_{m=1}^M \alpha_m G_m(x) \right)
  $$

由前向分步算法可以推导出 AdaBoost：
$$
f_m(x)=f_{m-1}(x)+\alpha_m G_m(x)
$$
目标函数：
$$
(\alpha_m,G_m(x))
= \arg\min_{\alpha,G} \sum_i \exp[-y_i(f_{m-1}(x)+\alpha G(x_i))]
$$
可以证明，当使用指数损失函数 $\exp(-y_if(x_i))$ 时，系数 $\alpha$ 能使损失函数最小。


## 随机森林