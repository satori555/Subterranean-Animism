# 支持向量机

支持向量机（support vector machine，SVM）是一种二分类模型，基本思想为在特征空间中寻找一个间隔最大的超平面，最终可以转化为一个凸二次优化的问题。

一般有三种情况：

+ 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机。
+ 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器。
+ 当训练数据线性不可分时，通过使用核函数技巧即软间隔最大化，学习非线性支持向量机。

### 求解目标

##### 间隔

给定超平面 $w\cdot x+b=0$ ，样本点 $x$ 到超平面的距离（即几何间隔）为
$$
\gamma=\frac{y(w \cdot x+b)}{||w||}
$$
其中 $w,x$ 是向量，$y=\pm1$ 代表类别。

我们希望得到所有样本 $i$ 都满足的最大的间隔，即
$$
\max_{w,b}\gamma,\quad 
s.t. \frac{y_i(w \cdot x_i+b)}{||w||} \ge \gamma
$$
由于 $\gamma,||w||$ 都是标量，上式等价于
$$
\max_{w,b}\frac1{||w||},\quad 
s.t. y_i(w\cdot x_i+b) \ge 1
$$
又等价于
$$
\min_{w,b}\frac12||w||^2,\quad 
s.t. y_i(w\cdot x_i+b) \ge 1
$$

##### 对偶问题

使用拉格朗日乘子法，引入 $\alpha_i\ge 0$ ：
$$
L(w,b,\alpha)=\frac12||w||^2-\sum_i\alpha_i(y_i(w\cdot x_i+b)-1)
$$
令 $\theta(w)=\max_{\alpha_i\ge0}L(w,b,\alpha)$ ，则

+ 当样本点不满足约束时，即 $y_i(w\cdot x_i+b)-1<0$ ，$\theta$ 为无穷大。

+ 当样本点满足约束时，即 $y_i(w\cdot x_i+b)-1\ge0$，$\theta=\frac12||w||^2$ 。

那么约束问题等价于：
$$
\min_{w,b}\theta(w)=\min_{w,b}\max_{\alpha_i\ge0}L(w,b,\alpha)
$$
如果先求最大值再求最小值，那么要先求解带有未知参数 $w,b$ 的方程，而 $\alpha$ 又是不等式约束，这个求解过程不好做。我们把求最大值和最小值的顺序交换一下，得到 **对偶问题**：
$$
\max_{\alpha_i\ge0}\min_{w,b}L(w,b,\alpha)
$$
原问题和对偶问题等价需要满足两个条件：

+ 优化问题是凸优化问题

+ 满足KKT条件
  $$
  \begin{cases}
  \alpha_i\ge0, \\
  y_i(w\cdot x_i+b)-1\ge0, \\
  \alpha_i(y_i(w\cdot x_i+b)-1)\ge0.
  \end{cases}
  $$
  

令 $L(w,b,\alpha)$ 对 $w,b$ 的偏导为 0 ：
$$
w=\sum_i\alpha_iy_ix_i \\
\sum_i\alpha_iy_i=0
$$
结果带入拉格朗日函数 $L(w,b,a)$ 并消去 $w,b$ ：
$$
\min_{w,b}L(w,b,a)=-\frac12\sum_i\sum_j\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) + \sum_i\alpha_i
$$
其中求和是对所有样本数据的操作。

再求对 $\alpha_i$ 的极大，等价于对目标函数加个负号然后求极小，得到最终的优化目标：
$$
\min_{\alpha_i}\left[ \frac12\sum_i\sum_j\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) - \sum_i\alpha_i \right] \\
s.t.\quad \sum_i\alpha_iy_i=0
$$
求解这个问题可以用序列最小优化（SMO）算法。

### 软间隔

实际情况下几乎不存在完全线性可分的数据，为此引入了 **软间隔** 的概念，即允许某些点不满足约束
$$
y_i(w\cdot x_i+b)\ge1
$$
采用 hinge loss，把原优化问题改写为：
$$
\min_{w,b,\xi} \frac12||w||^2+C\sum_i\xi_i
$$
其中 $\xi_i=\max(0,1-y_i(w\cdot x_i+b))$ 为 **松弛变量** ，即一个 hinge loss 损失函数，$C$ 为惩罚参数。

### 核函数

对于非线性分类问题，可以通过非线性变换 $x\to\phi(x)$ 将它转化为某个维特征空间中的线性分类问题，在高维特征空间中学习线性支持向量机。由于目标函数只涉及实例与实例之间的内积 $x_i\cdot x_j$ ，所以不需要显式地指定非线性变换，而是用核函数替换当中的内积：
$$
K(x_i,x_j)=\phi(x_i)\phi(x_j)
$$
在线性支持向量机的对偶问题中，用核函数 $K(x_i,x_j)$ 代替内积，求解得到的就是非线性支持向量机。



##### 参考

[1] 支持向量机（SVM）——原理篇 - 知乎
https://zhuanlan.zhihu.com/p/31886934

[2] 支持向量机通俗导论（理解SVM的三层境界）_网络_结构之法 算法之道-CSDN博客
https://blog.csdn.net/v_july_v/article/details/7624837

