# 机器学习常用的数学推导

## 优化方法

### 梯度下降法：

没啥好说的
$$
x_{n+1}=x_n-\eta\nabla f(x)
$$
其中$\eta$为学习率。

#### 基于梯度下降法的变式：

SGD：随机梯度下降，随机选一条或多条数据（mini-batch）进行梯度下降。

Momentum：为了减少震荡，每次更新的时候加上上一次更新的向量（乘一个0到1的系数），使得两次更新方向相同时加速，方向相反时减速。

AdaGrad：动态调整学习率，让全局学习率除以历史梯度的平方和再开根号，学习率逐步下降。

RMSProp：AdaGrad的改进版，梯度平方累加时加一个调整系数。

Adam：Momentum和RMSProp的结合。

### 牛顿法：

牛顿法可用于求解方程$g(x)=0$。

选取$x_0$作为初始值，过点$(x_0,g(x_0))$作切线$L$：
$$
L:y=g(x_0)+g'(x_0)(x-x_0)
$$
找到$L$与x轴的交点$x_1$：
$$
x_1=x_0-\frac{g(x_0)}{g'(x_0)}
$$
重复以上步骤，直到结果收敛。递推公式为：
$$
x_{n+1}=x_n-\frac{g(x_n)}{g'(x_n)}
$$
推广：求解无约束最优化问题：$\min\limits_x f(x)$

二阶泰勒展开：
$$
f(x)=f(x_k)+f'(x)(x-x_k)+\frac 12f''(x_k)(x-x_k)^2
$$
极小值满足$f'(x)=0$，即
$$
f'(x_k)+f''(x_k)(x-x_k)=0
$$

$$
x_{k+1}=x_k-\frac{f'(x_k)}{f''(x_k)} \equiv x_k-\frac{g_k}{h_k}
$$

推广到$\vec{x}$是多维向量的情况：
$$
\vec{g}=\left[\frac{\part f}{\part x_i}\right]
$$
$H$是Hessian矩阵：
$$
H=\left[\frac{\part^2f}{\part x_i\part x_j}\right]
$$
迭代公式变为：
$$
\vec x_{k+1} = \vec x_k - H_k^{-1}\vec g_k
$$

#### 牛顿法改进：

阻尼牛顿法：优化步长。

拟牛顿法：为了避免对Hessian矩阵求逆，尝试用一个不含二阶导数的矩阵$U$来代替$H^{-1}$，具体构造方法有DFP法，BFGS法，L-BFGS法等。



## 线性回归

给定数据集(x, y)，用一个线性函数 $f(x_i)=wx_i+b$ 拟合，使得$f(x_i)\approx y_i$。

最小二乘法确定参数。

## 逻辑回归（对数几率回归）

考虑二分类任务，把线性回归模型的预测值z转换为0或1，例如单位阶跃函数，若z>0判为正例，若z<0判为反例。由于阶跃函数不连续，可以采用对数几率函数(logistic function):
$$
\begin{align}
y &= \frac1{1+e^{-z}} \\
  &= \frac1{1+e^{-w^Tx+b}}
\end{align}
$$
（注意x和w是向量）
$$
\ln\frac y{1-y} = w^Tx+b
$$
到这里函数y的意义还不清楚，我们可以自己定义（这会影响到计算w）。若将y视为x为正例的可能性$p(y=1|x)$，则1-y就是反例的可能性$p(y=0|x)$，那么
$$
\ln\frac{p(y=1|x)}{p(y=0|x)}=w^Tx+b
$$
定义对数几率(log odds，或logit)：
$$
\text{logit}(y)=\log\frac y{1-y}
$$
正例和反例的概率为：
$$
p(y=1|x)\equiv \pi(x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}} \\
p(y=0|x)\equiv 1-\pi(x)=\frac1{1+e^{w^Tx+b}}
$$


给定数据集$\{\vec{x}_i,y_i\}$，使用极大似然法来估计参数。似然函数为：
$$
\prod_{i=1}^N[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}
$$
对数似然函数：
$$
\begin{align}
L(w) &=\sum_{i=1}^N[y_i\log\pi(x_i)+(1-y_i)log(1-\pi(x_i))] \\
&=\sum_{i=1}^N\left[ y_i\log\frac{\pi(x_i)}{1-\pi(x_i)}+\log(1-\pi(x_i)) \right] \\
&=\sum_{i=1}^N\left[ y_i(wx_i+b)-\log(1+e^{(wx_i+b)}) \right]
\end{align}
$$
其中$wx_i\equiv\sum_j w_jxji$

对$L(w)$求极大值得到$w$的估计值。



## BP神经网络

## 支持向量机

## 朴素贝叶斯分类器

## EM算法

## AdaBoost

## 树模型

决策树

xgboost

随机森林



