# 贝叶斯决策论（Bayes Decision Theory）

## 贝叶斯分类器

考虑抛硬币的例子：我们观察到的硬币尺寸为 X（大、中、小），可能的投掷结果为 Y：正(1)、反(0)。

##### 先验概率：

假如我们对硬币一无所知，我们只能预测正反面概率都是 0.5。又或者这是一种特殊的硬币，出现正面的概率 P(1)=0.8，出现反面的概率 P(0)=0.2，这样我们预测更容易出现正面。这里的 P(1) 和 P(0) 叫做 **先验概率** 。我们在观测（硬币尺寸 X）之前就已知的概率分布。

##### 后验概率：

实际上我们可以观测到硬币的一些属性，而非完全一无所知。因此我们尝试回答：当我观察到硬币大小时，它正面（反面）朝上的概率是多少？也就是估计 P(y=1|X)或 P(y=0|X)。

这里 P(y|X) 叫做 **后验概率** ，指的是观测到 X 后我们对 y 的估计。

后验概率引入了额外的观测信息，所以预测的准确度得到了加强。大部分机器学习模型尝试得到的，就是后验概率。

##### 贝叶斯公式：

后验概率无法直接获得，需要借助贝叶斯公式：
$$
P(y|X)=\frac{P(X|y)P(y)}{P(X)}
$$
等号右边，P(y) 为先验概率，P(X) 为观测结果，P(X|y) 叫做 **类条件概率** ，也可以通过观测获得。

有了后验概率 P(y|X)，就可以对样本进行分类。

## 极大似然估计（maximum likelihood estimation）

实际上由于我们只能获得有限的数据，先验概率 P(y) 和类条件概率 P(X|y) 都是未知的，因此我们要先进行估计。

先验概率的估计比较简单，可以依靠经验，或者用训练数据中各个类别的频率估计。

类条件概率的估计非常难，解决的办法是把估计完全未知的概率分布 P(X|y) 转化为估计参数，可以采用极大似然估计方法。

##### 极大似然估计：

先确定模型（概率分布），模型的参数未知。利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。

假设样本都是独立同分布，考虑样本集 $D(x_1,...,x_N)$ ，估计参数向量 $\theta$ 。

似然函数：
$$
l(\theta)=p(D|\theta)=\prod_i p(x_i|\theta)
$$
求出使该组样本的概率最大的参数 $\theta$ 值：
$$
\hat\theta=\arg\max_\theta l(\theta)
$$
为了避免连乘，定义对数似然函数 $\ln l(\theta)$ ：
$$
\hat\theta=\arg\max_\theta\sum_i\ln p(x_i|\theta)
$$
求导，解似然方程。



参考：

[1] 极大似然估计详解，写的太好了！_qq_39355550的博客-CSDN博客
https://blog.csdn.net/qq_39355550/article/details/81809467

## EM算法

要估计的似然函数包含隐变量 $z$ ：$L(\theta)=\ln p(x,z|\theta)$ 。

EM算法的关键是找到 $L(\theta)$ 的一个下界 $Q(\theta;\theta^n)$ ，然后不断极大化这个下界，从而逼近要求解的似然函数。

EM算法的一般步骤：

+ 随机初始化 $\theta^0$ 
+ 迭代以下两步
  + E步：给出当前的参数估计 $\theta^n$ ，计算似然函数的下界 $Q(\theta;\theta^n)$ 
  + M步：重新估计参数，即求 $\theta^{n+1}$，使得 $\theta^{n+1}=\arg\max_\theta Q(\theta;\theta^n)$ 
+ 直到 $Q(\theta;\theta^n)$ 收敛

##### 举个例子

假设有 A、B 两枚硬币，正面朝上的概率分别为 $\theta_A$ 和 $\theta_B$ ，这两个参数即为需要估计的参数。

我们设计 5 组实验，每次实验投掷 5 次硬币，但是每次实验都不知道用哪一枚硬币进行的本次实验。投掷结束后，会得到一个数组 $x=(x_1,x_2,...,x_5)$ ，表示每组实验有几次硬币正面朝上，因此 $0\le x_i\le5$ 。

如果我们知道每组实验中 $x_i$ 是 A 硬币的结果还是 B 硬币的结果，我们就可以估算 $\theta_A$ 和 $\theta_B$ ，只需要分别统计正面朝上的次数，然后除以投掷的总次数。

假设 5 次实验结果如下：

| 硬币 | 正面次数 | 反面次数 |
| ---- | -------- | -------- |
| ？   | 3        | 2        |
| ？   | 2        | 3        |
| ？   | 1        | 4        |
| ？   | 3        | 2        |
| ？   | 2        | 3        |

EM算法流程：

+ 首先，随机初始化参数，例如 $\theta_A=0.2$ ，$\theta_B=0.7$ 。

+ EM算法的 E 步：计算在当前预估参数下，隐变量（是哪个硬币）的每个值出现的概率。

  对于第一组实验：

  + 用 A 硬币得到这个结果的概率为：$0.2^3\times 0.8^2=0.00512$ 
  + 用 B 硬币得到这个结果的概率为：$0.7^3\times 0.3^2=0.03087$  

  因此，第一组实验用的是 A 硬币的概率为 $0.00512/(0.00512+0.03087)=0.14$ ，用的是 B 硬币的概率为 $0.03087/(0.00512+0.03087)=0.86$ 。整个 5 组实验的硬币概率如下：

  | 实验 | A硬币 | B硬币 |
  | :--: | :---: | :---: |
  |  1   | 0.14  | 0.86  |
  |  2   | 0.61  | 0.39  |
  |  3   | 0.94  | 0.06  |
  |  4   | 0.14  | 0.86  |
  |  5   | 0.61  | 0.39  |

+ EM算法的 M 步：利用最大似然估计法重新估计参数。

  我们根据 E 步得到的硬币概率重新计算期望。例如第一组 3 正 2 反，如果是 A 硬币的结果，期望为 $0.14\times3=0.42$ 个正面和 $0.14\times2=0.28$ 个反面，如果是 B 硬币的结果，期望为 $0.86\times3=2.58$ 个正面和 $0.86\times2=1.72$ 个反面，合计 3 正 2 反。

  统计全部期望值：

  | 实验 | 正面    反面 （A） | 正面   反面（B） |
  | :--: | :----------------- | :--------------- |
  |  1   | 0.42    0.28       | 2.58    1.72     |
  |  2   | 1.22    1.83       | 0.78    1.17     |
  |  3   | 0.94    3.76       | 0.06    0.24     |
  |  4   | 0.42    0.28       | 2.58    1.72     |
  |  5   | 1.22    1.83       | 0.78    1.17     |
  | 总计 | 4.22    7.98       | 6.78    6.02     |

  重新估计 $\theta_A$ 和 $\theta_B$ ：
  
  $\theta_A=4.22/(4.22+7.98)=0.35$ 
  
  $\theta_B=6.78/(6.78+6.02)=0.53$ 
  
+ 重复 E 步和 M 步直到收敛。

参考：

[1] EM算法详解 - 知乎
https://zhuanlan.zhihu.com/p/40991784