# Variational Auto-Encoders (VAE)

基本的自编码器本质上是学习输入 $x$ 与隐变量 $z$ 之间的映射关系，是一个判别式模型。

变分自编码器是一个生成式模型，即希望构建一个从隐变量 $z$ 生成目标数据 $x$ 的模型。

### VAE理解

包含隐变量的分布：
$$
p(x)=\sum_zp(x|z)p(z)
$$
假设 $z$ 服从标准正态分布，然后就可以采样 $z$ 得到 $x$ 。

但是这样我们不知道真实样本和生成样本的对应关系，所以实际上是这样的：

给定一个样本 $x_k$ ，假设存在一个后验分布 $p(z|x_k)$ ，并进一步假设这个分布是（独立的、多元的）正态分布。这些正态分布的均值和方差可以用神经网络拟合出来。

为了保证模型的生成能力，还需要让所有的正态分布向标准正态分布看齐。这样就得到了一开始的假设，即 $p(z)$ 是标准正态分布，这样就可以从 $N(0,I)$ 中采样来生成图像了。

为了向正态分布看齐，可以在误差函数中加入正则项，例如 KL 散度：
$$
D_{KL}\left(N(\mu,\sigma^2) \| N(0,I)\right) \\
= \frac12\sum_{i=1}^d\left( \mu_i^2+\sigma_i^2 - \log{\sigma_i^2 -1} \right)
$$
由于 KL 散度是大于等于 0 的，实际上的优化目标是在逼近这个界限，变分的思想就体现在这里，具体看下一节推导。

### 推导

设想我们用一个分布 $q_\phi(z|x)$ 来逼近真实的分布 $p(z|x)$ ：
$$
\min_\phi D_{KL}\left( q_\phi(z|x) \big\| p(z|x) \right)
$$

$$
\begin{align}
D_{KL} &= \int_z{q_\phi(z|x)\log\frac{q_\phi(z|x)}{p(z|x)} dz}\\
&= \int_z{q_\phi(z|x)\log\frac{q_\phi(z|x)p(x)}{p(x,z)} dz} \\
&= \int_z{q_\phi(z|x)\log\frac{q_\phi(z|x)}{p(x,z)} dz} + \int_z{q_\phi(z|x)\log{p(x)} dz} \\
&= -\left(-\int_z{q_\phi(z|x)\log\frac{q_\phi(z|x)}{p(x,z)} dz}\right) + \log{p(x)}
\end{align}
$$

最后括号里的一项就可以定义为目标函数：
$$
L(\phi,\theta) = -\int_z{q_\phi(z|x)\log\frac{q_\phi(z|x)}{p(x,z)} dz}
$$
因为 KL 散度大于等于0，所以：
$$
L(\phi,\theta) \le \log{p(x)}
$$
也就是说，目标函数 $L$ 是 $\log(x)$ 的下界，被称为Evidence Lower Bound Objective (ELBO)。我们的目标是优化 $p(x)$ ，也就可以通过优化 $L$ 来实现：
$$
\begin{align}
L(\phi,\theta) &= -\int_z{q_\phi(z|x)\log\frac{q_\phi(z|x)}{p(x,z)} dz} \\
&= \int_z{q_\phi(z|x)\log\frac{p(z)p_\theta(x|z)}{q_\phi(z|x)} dz} \\
&= \int_z{q_\phi(z|x)\log\frac{p(z)}{q_\phi(z|x)} dz} + \int_z{q_\phi(z|x)\log{p_\theta(x|z)} dz} \\
&= -D_{KL}\left( q_\phi(z|x) \big\| p(z) \right) + \mathbb{E} _{z\sim q}[\log{p_\theta(x|z)}] \\


\end{align}
$$
这就是一般看到的 VAE 的目标函数。可以用 Encoder 网络参数化 $q_\phi(z|x)$ ，用 Decoder 网络参数化 $p_\theta(x|z)$。一般来说 $q_\phi(z|x)$ 是正态分布，$p(z)$ 是标准正态分布。可以结合上一节理解。

### Reparameterization Trick

由于 Decoder 的输入 $z$ 是采样得到的，这个操作不可导，导致无法通过梯度下降法训练网络。

重参数化技巧：从 $N(\mu,\sigma)$ 中采样一个 $z$ ，相当于从 $N(0,I)$ 中采样一个 $\epsilon$ ，然后让 $z=\mu+\epsilon \times \sigma$ 。这样采样的操作就不参与梯度下降了。



**参考**：

[1] 变分自编码器（一）：原来是这么一回事 - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/5253

[2] GitHub - dragen1860/Deep-Learning-with-TensorFlow-book: 深度学习入门开源书，基于TensorFlow 2.0案例实战。Open source Deep Learning book, based on TensorFlow 2.0 framework.
https://github.com/dragen1860/Deep-Learning-with-TensorFlow-book