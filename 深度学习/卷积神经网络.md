#  Convolutional  Neural Network (CNN)

### 基本结构

输入层

卷积层（convolutional layer）

池化层（pooling layer）

（flatten）

全连接层



### 卷积核

##### Conv1D

常用于文本等。卷积核宽度和词向量长度一样，另一维度类似ngram提取文本信息。池化后保留最大的数字，然后和其他卷积核的结果拼接起来，结果输出到全连接层。

##### Conv2D

最典型的卷积神经网络，用于单张静态图像。shape=(L, W, 1)

对于多通道图像（如RGB），每个通道都有一个卷积核，对每个通道完成卷积后把结果相加得到一个特征。shape=(L, W, 3)

##### Conv3D

常用于视频等，多了一个depth维度，对于RGB视频，shape=(L, W, D, 3)。

例如数据大小为(a, b, c)，卷积核大小为(f, f, f)，数量为n，一次卷积操作后得到n个大小为(a-f+1, b-f+1, c-f+1)的数据。

##### 填充（Padding）

卷积操作后图像会变小，如果想保持原来的大小可以在周围补一圈0。另外卷积移动的步长也可以调节。

##### 池化

对特征进行压缩，一般保留最大的那个数。

##### Flatten

把二维数组转换成一维（按行输出），然后输入全连接层。



### 误差逆传递

有点复杂，有空再补充。



### 一些例子

##### LeNet-5

非常经典的用于手写体字符识别的卷积神经网络。



##### VGG16

该模型参加2014年的 ImageNet图像分类与定位挑战赛，取得了优异成绩：在分类任务上排名第二，在定位任务上排名第一。 包含13个卷积层和3个全连接层。



##### ResNet 残差网络

当深度CNN达到一定深度后再增加层数性能反而会下降，而且会使网络收敛更慢，然后出现了深度残差学习。

当下Resnet已经代替VGG成为一般计算机视觉领域问题中的基础特征提取网络。 

https://www.jianshu.com/p/93990a641066



### 参考：

[1] https://blog.csdn.net/weixin_42451919/article/details/81381294

[2] https://www.zybuluo.com/hanbingtao/note/485480

[3] https://blog.csdn.net/jiaoyangwm/article/details/80011656