# 语言模型

## one-hot

缺点：维度高，稀疏，正交（任意两个词距离都是1）

我们希望语义相近的词距离比较近，语义不相近的词距离比较远。

## n-gram

不能依赖较长的上下文

## 神经网络语言模型



## word2vec

分布式假设：如果两个词的上下文是相似的，那么它们的语义也是相似的。

word2vec分为cbow（根据context预测中心词）和skip-gram（根据中心词预测context）两种。

缺点：多义性

## RNN/LSTM/GRU



## attention机制



## transformer

self-attention

## BERT



参考：

从 one-hot 到 BERT，带你一步步理解 BERT_概率
http://www.sohu.com/a/322817228_294584