# 语言模型

自然语言从它产生开始，逐渐演变成一种上下文相关的信息表达和传递的方式，因此让计算机处理自然语言，一个基本的问题就是为自然语言这种上下文相关的特性建立数学模型。这个数学模型就是在自然语言处理中常说的统计语言模型。——《数学之美》

### TF-IDF

词袋模型（bag of words，BOW）。

定义权重：

+ 布尔权重
+ 特征频率（term frequency，TF）：表示该特征项（词）在当前文本中出现的次数。
+ 倒文档频率（inverse document frequency，IDF）：文档频率（DF）表示语料中包含某个词的文档的数目。一个词的DF越高，其包含的有效信息量往往越低。
+ TF-IDF权重：定义为TF和IDF的乘积。TF-IDF认为，对区别文本最有意义的特征项应该是那些在当前文本中出现的频率足够高，而在其他文本中出现的频率足够小的词语。



### n元语法模型（n-gram）

以n个字组成的词组作为基本单元，可以捕捉到一部分词序信息。

例如二元模型，下一个词出现的概率只和前一个词有关：
$$
P(w_i|w_{i-1})=\frac{P(w_{i-1},w_i)}{P(w_{i-1})}\approx\frac{\#(w_{i-1},w_i)}{\#(w_{i-1})}
$$
缺点是随着n的增加，特征空间的维数呈指数级增长$O(|V|^n)$，特征向量变得愈加稀疏。

### 词的分布式表示

分布式表示学习的核心思想就是利用低维连续的实数向量（词向量）表示一个词语，使得语义相近的词在词向量空间中也临近。



### 神经网络语言模型

前馈神经网络语言模型：Bengio et al., 2003

将每个词映射为一个低维连续的词向量，并在连续向量空间中对n元语言模型的概率$P(w_i|w_{i-n+1},...,w_{i-1})$进行建模。首先历史信息的（n-1）个词被映射为词向量，拼接后得到 $h_0$ ，然后通过隐藏层学习这（n-1）个词的抽象表示，最后用SoftMax计算词表中每个词的概率分布。

由于前馈神经网络仅能对固定窗口的上下文进行建模，无法捕捉长距离的上下文依赖关系，Mikolov(2010)等人采用了循环神经网络进行建模。

此外还有卷积网络语言模型(Dauphin et al., 2016)。



参考：

[1] 宗成庆，文本数据挖掘，2019

[2] https://www.jianshu.com/p/a02ea64d6459

### word2vec

一般分为CBOW(Continuous Bag-of-Words)与Skip-Gram两种模型。

CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。　

Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。

CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。

##### CBOW(Continuous Bag-of-Words)模型

在神经网络语言模型中，词向量的表示学习只是为了建立语言模型得到的副产品。从C&W开始，就直接以生成词向量为目标构建模型了。

C&W的基本思想是，给定训练语料中任意一个n元组（n=2C+1），如果把中心词随机替换成词表中别的词，那么真正的词一定比随机的词更合理，进行打分的话就是真实的中心词得分更高。训练过程就是拼接上下文词向量，然后送到隐藏层，最后经过线性变换得到一个分数。相比神经网络语言模型，最后一层计算量由$O(|h|\times|V|)$下降到$O(|h|)$。

CBOW做了两点简化，一个是输入层不再拼接，而是取平均值，另外没有隐藏层，而是用Logistic回归计算中心词的概率。

给定一个n元组 $(w_i,C)=w_{i-C},...,w_{i-1},w_i,w_{i+1},...,w_{i+C}$ ，将 $WC=w_{i-C},...,w_{i-1},w_{i+1},...,w_{i+C}$ 作为输入，上下文平均词向量为：
$$
h=\frac1{2C}\sum_{i-C\le k\le i+C}e(w_k)
$$
中心词 $w_i$ 的概率为：
$$
P(w_i|WC)=\frac{\exp(h\cdot e(w_i))}{\sum_{k=1}^{|V|}\exp(h\cdot e(w_k))}
$$
训练时，对整个训练语料，优化词向量矩阵 $L$，最大化所有词的对数似然：
$$
L^*=\arg\max_L\sum_{w_i\in V}\log P(w_i|WC)
$$


##### Skip-gram模型

给定一个n元组 $(w_i,C)$ ，利用中心词 $w_i$ 的词向量 $e(w_i)$ 预测上下文 $WC=w_{i-C},...,w_{i-1},w_{i+1},...,w_{i+C}$ 中每个词 $w_c$ 的概率：
$$
P(w_c|w_i)=\frac{\exp(e(w_i)\cdot e(e_c))}{\sum_{k=1}^{|V|}\exp(e(w_i)\cdot e(e_k))}
$$
目标函数与CBOW类似，优化词向量矩阵以最大化所有上下文词的对数似然：
$$
L^*=\arg\max_L\sum_{w_i\in V}\sum_{w_c\in WC}\log P(w_c|w_i)
$$


参考：https://www.jianshu.com/p/471d9bfbd72f



## BERT

##### Embedding

Bert的词嵌入由三种embedding求和得到。

+ Token embedding：词向量，第一个单词是[CLS]标志，可以用于之后的分类任务
+ Segment embedding：文本向量，用来刻画文本的全局语义信息
+ Position embedding：位置向量，对不同位置的词附加一个不同的向量加以区分



##### 预训练1：Masked LM

在一句话中随机抹去15%的词用于预测，而不是像CBOW一样把每个词都预测一遍。

对于被抹去的词，80%用一个特殊符号[MASK]替换，10%用一个任意词替换，剩下10%保持不变。

步骤：

1. 在encoder的输出上添加一个分类层
2. 用嵌入矩阵乘以输出向量，将其转换为词汇的维度
3. 用SoftMax计算词汇表中每个单词的概率

由于一次只预测15%，所以收敛比较慢。



##### 预训练2：Next sentence prediction

在训练过程中，接收成对的句子作为输入，并预测第二个句子是否在原始文档中也是后续句子。训练期间，50%的输入在原始文档中是前后关系，另外50%是从语料库中随机组成的，并且是与第一句断开的。

为了区分这两个句子，输入在进入模型之前要做以下处理：

1. 在第一个句子开头插入[CLS]，在末尾插入[SEP]
2. 将表示句子的embedding（文本向量）添加到每个词token上
3. 给每个token添加一个位置embedding，来表示它在序列中的位置。

预测时用一个分类层将[CLS]标记的输出变换为2*1形状的向量，用SoftMax计算概率。



训练BERT模型时，Masked LM和Next Sentence Prediction一起训练，目标是最小化两种策略的组合损失函数。



##### Encoder

使用transformer encoder进行特征提取。作者分别用12层和24层encoder组装了两套BERT模型，参数分别为110M和340M。



参考：

[1] 图解BERT模型：从零开始构建BERT https://cloud.tencent.com/developer/article/1389555

[2] 5 分钟入门 Google 最强NLP模型：BERT https://www.jianshu.com/p/d110d0c13063