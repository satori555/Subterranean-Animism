# 语言模型

## one-hot

缺点：维度高，稀疏，正交（任意两个词距离都是1）

我们希望语义相近的词距离比较近，语义不相近的词距离比较远。

## n-gram

不能依赖较长的上下文

## 神经网络语言模型



## word2vec

分布式假设：如果两个词的上下文是相似的，那么它们的语义也是相似的。

word2vec分为cbow（根据context预测中心词）和skip-gram（根据中心词预测context）两种。

缺点：多义性

## RNN/LSTM/GRU



## attention机制



参考：

https://www.jianshu.com/p/c94909b835d6



## transformer

self-attention

## BERT

Bert的三个亮点：

Masked LM, Transformer, sentence-level



参考：

[1] 从 one-hot 到 BERT，带你一步步理解 BERT_概率 http://www.sohu.com/a/322817228_294584

[2] https://www.cnblogs.com/rucwxb/p/10277217.html

[3] https://www.jianshu.com/p/d110d0c13063