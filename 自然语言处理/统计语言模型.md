# 统计语言模型

自然语言从它产生开始，逐渐演变成一种上下文相关的信息表达和传递的方式，因此让计算机处理自然语言，一个基本的问题就是为自然语言这种上下文相关的特性建立数学模型。这个数学模型就是在自然语言处理中常说的统计语言模型。——《数学之美》

### TF-IDF

词袋模型（bag of words，BOW）。

定义权重：

+ 布尔权重
+ 特征频率（term frequency，TF）：表示该特征项（词）在当前文本中出现的次数。
+ 倒文档频率（inverse document frequency，IDF）：文档频率（DF）表示语料中包含某个词的文档的数目。一个词的 DF 越高，其包含的有效信息量往往越低。
+ TF-IDF 权重：定义为 TF 和 IDF 的乘积。TF-IDF 认为，对区别文本最有意义的特征项应该是那些在当前文本中出现的频率足够高，而在其他文本中出现的频率足够小的词语。



### n 元语法模型（n-gram）

以 n 个字组成的词组作为基本单元，可以捕捉到一部分词序信息。

例如二元模型，下一个词出现的概率只和前一个词有关：
$$
P(w_i|w_{i-1})=\frac{P(w_{i-1},w_i)}{P(w_{i-1})}\approx\frac{\#(w_{i-1},w_i)}{\#(w_{i-1})}
$$
缺点是随着n的增加，特征空间的维数呈指数级增长 $O(|V|^n)$，特征向量变得愈加稀疏。

### 词的分布式表示

分布式表示学习的核心思想就是利用低维连续的实数向量（词向量）表示一个词语，使得语义相近的词在词向量空间中也临近。



### 神经网络语言模型

前馈神经网络语言模型：Bengio et al., 2003

将每个词映射为一个低维连续的词向量，并在连续向量空间中对n元语言模型的概率$P(w_i|w_{i-n+1},...,w_{i-1})$进行建模。首先历史信息的（n-1）个词被映射为词向量，拼接后得到 $h_0$ ，然后通过隐藏层学习这（n-1）个词的抽象表示，最后用 Softmax 计算词表中每个词的概率分布。

由于前馈神经网络仅能对固定窗口的上下文进行建模，无法捕捉长距离的上下文依赖关系，Mikolov(2010) 等人采用了循环神经网络进行建模。

此外还有卷积网络语言模型(Dauphin et al., 2016)。



##### 参考：

[1] 宗成庆，文本数据挖掘，2019

[2] https://www.jianshu.com/p/a02ea64d6459

