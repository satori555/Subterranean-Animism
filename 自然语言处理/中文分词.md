# 中文分词

### 基于词典的分词算法：

1. 正向最大匹配，从左到右查找，匹配到的词越长越好。
2. 逆向最大匹配，从右到左查找，匹配到的词越长越好。
3. 双向匹配，选择二者中分词数较少的。

### 基于统计的分词算法：

BEMS标注：B 词开头的字，E 词最后一个字，M 词中间的字，S 单字词。

1. HMM，观测序列，隐藏序列，隐藏态起始概率，隐藏态转移概率，发射概率。
2. CRF
3. 深度学习

### jieba分词原理

jieba分词综合了基于字符串匹配的算法和基于统计的算法，其分词步骤为

1. 初始化。加载词典文件，获取每个词语和它出现的词数
2. 切分短语。利用正则，将文本切分为一个个语句，之后对语句进行分词
3. 构建DAG。通过字符串匹配，构建所有可能的分词情况的有向无环图，也就是DAG
4. 构建节点最大路径概率，以及结束位置。计算每个汉字节点到语句结尾的所有路径中的最大概率，并记下最大概率时在DAG中对应的该汉字成词的结束位置。
5. 构建切分组合。根据节点路径，得到词语切分的结果，也就是分词结果。
6. HMM新词处理：对于新词，也就是dict.txt中没有的词语，我们通过统计方法来处理，jieba中采用了HMM隐马尔科夫模型来处理。
7. 返回分词结果：通过yield将上面步骤中切分好的词语逐个返回。yield相对于list，可以节约存储空间。

### HMM新词处理

+ 观测序列：语句本身，我们能看见的
+ 隐藏序列：由BMES构成的分词标注序列，比如“有意见分歧”对应的标注有两种，为SBEBE和BESBE，分别对应分词序列“有/意见/分歧”和“有意/见/分歧”。
+ 发射概率：隐藏值到观测值的概率，比如S是汉字“有”的概率。
+ 起始概率：隐藏值起始概率，起始只能是B或者S，通过语料大规模训练可以得到B和S作为起始的概率。结果为{‘B’: 0.769, ‘E’: 0, ‘M’: 0, ‘S’: 0.231}，可见起始为B的概率要远大于S，这也符合我们通常情况。
+ 转移概率：隐藏值之间转移的概率，比如B->E, 表示为P(E|B), B->M, 表示为p(M|B)

通过语料大规模训练，可以得到发射概率，起始概率和转移概率。通过viterbi算法，可以得到概率最大的隐藏序列，也就是 BEMS标注序列，通过BEMS就可以对语句进行分词了。



参考：

[1] 自然语言处理1 -- 分词人工智能谢杨易的博客-CSDN博客
https://blog.csdn.net/u013510838/article/details/81673016

[2] 自然语言处理2 -- jieba分词用法及原理人工智能谢杨易的博客-CSDN博客
https://blog.csdn.net/u013510838/article/details/81738431